{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from itertools import permutations\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from torch import nn, optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper functions/classes:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Id = lambda x: x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SymmDataGenerator:\n",
    "    n_features: int\n",
    "    group_elems: list\n",
    "    labeller: callable\n",
    "    domain: str = 'hypercube'\n",
    "    noise: float = None\n",
    "    classification: bool = False\n",
    "    cutoff: float = 1.\n",
    "    # whether to explicitly input all the elements in an orbit or have the symmetry be in the data only on average\n",
    "    symmetrise_expl: bool = True\n",
    "    postprocess_func: callable = Id\n",
    "    # required in cases where the postprocessing adds/removes features\n",
    "    n_base_features: int = None\n",
    "\n",
    "    def __call__(self, n_samples: int, *args, **kwargs) -> tuple:\n",
    "        n_base_samples = n_samples // (len(self.group_elems) if self.symmetrise_expl else 1)\n",
    "\n",
    "        # need to be finnicky about what gets labelled when because the postprocessed data might not have a group\n",
    "        # representation - the symmetrisation works on the base data\n",
    "        base_data = self.gen_asymm_data(n_base_samples, postprocess=False)\n",
    "\n",
    "        labels = self.labeller(self.postprocess_func(base_data))\n",
    "\n",
    "        return self.symmetrise(base_data, labels) if self.symmetrise_expl else (base_data, labels.reshape(-1, 1))\n",
    "\n",
    "    def gen_asymm_data(self, n_samples: int, postprocess: bool = True) -> np.ndarray:\n",
    "        n_features = self.n_base_features if self.n_base_features is not None else self.n_features\n",
    "        if self.domain == 'hypercube':\n",
    "            data = 2 * (np.random.rand(n_samples, n_features) - 0.5)\n",
    "\n",
    "            return data if not postprocess else self.postprocess_func(data)\n",
    "\n",
    "        raise NotImplementedError('Still need to implement more general domains!')\n",
    "\n",
    "    def symmetrise(self, base_data: np.ndarray, labels: np.ndarray = None, noise: float = 0.,\n",
    "                   postprocess: bool = True) -> Union[tuple, np.ndarray]:\n",
    "        if self.noise is not None and noise == 0.:\n",
    "            noise = self.noise\n",
    "        data = np.concatenate([group_elem(base_data) for group_elem in self.group_elems])\n",
    "        data = data if not postprocess else self.postprocess_func(data)\n",
    "        if labels is not None:\n",
    "            labels = np.concatenate([labels] * len(self.group_elems))\n",
    "\n",
    "            if len(labels.shape) == 1:\n",
    "                labels = labels.reshape(-1, 1)\n",
    "\n",
    "            labels += np.random.randn(*labels.shape) * noise\n",
    "\n",
    "            # TODO - make sure this makes sense in noisy cases\n",
    "            if self.classification:\n",
    "                labels = labels > self.cutoff\n",
    "\n",
    "            return data, labels\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "AbsXSymmGenerator = SymmDataGenerator(1, [Id, lambda x: -x], lambda x: abs(x))\n",
    "XSquareSymmGenerator = SymmDataGenerator(1, [Id, lambda x: -x], lambda x: x ** 2)\n",
    "PermXYSymmGenerator = SymmDataGenerator(2, [Id, lambda x: x[:, [1, 0]]], lambda x: x.sum(axis=-1))\n",
    "\n",
    "\n",
    "def poly_kernel(features, order):\n",
    "    # returns 1,x,y,x**2,y**2,xy,etc.\n",
    "    n_samples, n_features = features.shape\n",
    "    kernelised_features = np.empty((n_samples, (order + 1) * (order + 2) // 2))\n",
    "\n",
    "    for i in range(order + 1):\n",
    "        for j in range(i + 1):\n",
    "            kernelised_features[:, i * (i + 1) // 2 + j] = features[:, 0] ** j * features[:, 1] ** (i - j)\n",
    "\n",
    "    return kernelised_features\n",
    "\n",
    "\n",
    "PolyXYSymmGenerator = lambda order: SymmDataGenerator(n_features=(order + 1) * (order + 2) // 2, n_base_features=2,\n",
    "                                                      group_elems=[Id, lambda x: x[:, [1, 0]]],\n",
    "                                                      labeller=lambda x: x.sum(axis=-1),\n",
    "                                                      postprocess_func=lambda features: poly_kernel(features, order))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_data_loader(data_generator, dataset_size, batch_size) -> DataLoader:\n",
    "    X, y = data_generator(dataset_size)\n",
    "\n",
    "    X = torch.Tensor(X).to(DEVICE)\n",
    "    X = torch.flatten(X, start_dim=1, end_dim=-1)\n",
    "    y = torch.Tensor(y).to(DEVICE)\n",
    "    dataset = TensorDataset(X, y)\n",
    "\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def calc_asymm_loss(model: nn.Module, data_gen: SymmDataGenerator, n_points: int = 100) -> float:\n",
    "    with torch.no_grad():\n",
    "        asymm_loss = 0\n",
    "        for _ in range(n_points):\n",
    "            x = data_gen.gen_asymm_data(1, postprocess=False)\n",
    "            symm_x = data_gen.symmetrise(x, postprocess=True)\n",
    "\n",
    "            x = torch.Tensor(data_gen.postprocess_func(x)).to(DEVICE)\n",
    "            symm_x = torch.Tensor(symm_x).to(DEVICE)\n",
    "\n",
    "            pred = model(x)\n",
    "            avg_symm_pred = model(symm_x).mean()\n",
    "\n",
    "            if data_gen.classification:\n",
    "                pred = torch.sigmoid(pred)\n",
    "                avg_symm_pred = torch.sigmoid(avg_symm_pred)\n",
    "\n",
    "            asymm_loss += abs(pred - avg_symm_pred).item()\n",
    "\n",
    "    return asymm_loss / n_points\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation: nn.modules.activation = nn.ReLU(), bias: bool = True):\n",
    "        super(Perceptron, self).__init__()\n",
    "\n",
    "        self.layer = nn.Linear(input_dim, output_dim, bias=bias)\n",
    "        self.act = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.layer(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_l_size: int, n_hidden: int, output_size: int,\n",
    "                 activation: nn.modules.activation = nn.ReLU(), bias: bool = True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        assert n_hidden >= 1, 'Error - you need some hidden layers for this to be an MLP!'\n",
    "\n",
    "        layers = [Perceptron(input_size, hidden_l_size, activation=activation, bias=bias)]\n",
    "        layers += [Perceptron(hidden_l_size, hidden_l_size, activation=activation, bias=bias)\n",
    "                   for _ in range(n_hidden - 1)]\n",
    "        layers += [nn.Linear(hidden_l_size, output_size, bias=bias)]\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        y = self.layers(x)\n",
    "        return y\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model definition:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_hidden = 1\n",
    "hidden_size = 10 ** 2\n",
    "n_samples = 10 ** 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "symm_data_gen = SymmDataGenerator(n_features, [lambda x: x], lambda x: x.sum(axis=-1))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = MLP(input_size=symm_data_gen.n_features, output_size=1, n_hidden=n_hidden, hidden_l_size=hidden_size)\n",
    "n_params = sum(p.numel() for p in model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculations:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ewk = torch.zeros((n_params, n_params)).float()\n",
    "\n",
    "dataloader = get_data_loader(symm_data_gen, n_samples, batch_size=1)\n",
    "\n",
    "for (X, y) in tqdm(dataloader):\n",
    "    model.zero_grad()\n",
    "\n",
    "    out = model(X)\n",
    "    out.backward()\n",
    "\n",
    "    grads = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
    "\n",
    "    ewk += torch.outer(grads, grads)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eig = torch.linalg.eig(ewk)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fs = 16"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_loghist(x, bins):\n",
    "    hist, bins = np.histogram(x, bins=bins)\n",
    "    logbins = np.logspace(np.log10(bins[0]), np.log10(bins[-1]), len(bins))\n",
    "    plt.hist(x, bins=logbins, log=True)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "\n",
    "\n",
    "plot_loghist(abs(eig.eigenvalues)[eig.eigenvalues != 0], 30)\n",
    "\n",
    "plt.ylabel('# of eigenvalues', fontsize=fs)\n",
    "plt.xlabel('Eigenvalue', fontsize=fs)\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs);\n",
    "\n",
    "plt.gcf().savefig('ewk_eig_spect.pdf', dpi=300, bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "t=epoch * learning rate, decay=exp(-eigenvalue*t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(abs(eig.eigenvalues) < 0.2).numpy().mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
